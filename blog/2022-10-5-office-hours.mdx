---
title: KubeSlice Office Hours - 25th August 2022
description: This is my first post on Docusaurus 2.
slug: welcome-docusaurus-v2-one
authors:
  - name: Tanisha Banik
    url: https://github.com/26tanishabanik
hide_table_of_contents: false
---

The [KubeSlice project](https://github.com/kubeslice) recently hosted the second episode of its Office Hours series on Thursday, 25th August 2022. 
This project, which was just open sourced by [Avesha](https://avesha.io/), brings together the entire community every other Thursday 
to learn more about different facets of KubeSlice. Visit our [YouTube channel](https://www.youtube.com/channel/UCtMSqfUzbv33CasQ8mzlSPw) to watch recordings of earlier events, 
including our very first office hours.

The configuration of **KubeSlice** and how it can be used to give multi-tenancy capabilities in a multi-cluster 
Kubernetes environment were covered in this episode by [Prabhu Navali](https://www.linkedin.com/in/prabhu-navali-13795a1/), director of engineering at Avesha.

Based on the concept of dividing a cluster to let different teams inside an organization use a dedicated set of resources, 
KubeSlice uses pre-existing Kubernetes primitives to define tenancy. With the aid of an overlay network, on which 
you may specify namespaces and apps, it is possible to construct a wider workspace over many clusters for a number of 
applications that can easily span between different clusters.

In addition to the above, KubeSlice also takes into account the following limitations while providing multi-tenancy 
in a multi-cluster Kubernetes environment:

- How can clusters that are dispersed across numerous cloud providers, geographies, and edges be connected?
- How can the same multi-tenancy be maintained throughout these clusters?

To learn more about how it achieves this, refer to the [KubeSlice documentation](https://kubeslice.io/).

With this foundation laid, Prabhu demonstrated the inner workings of KubeSlice with the help of two use cases.

If you want to follow along with the demo, the prerequisites are listed below: 
 
- Command Line Tools: 
  - [Helm](https://helm.sh/) - The Package Manager for Kubernetes, [Installation Documentation](https://helm.sh/docs/intro/install/)
  - [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/) - Kubernetes CLI, [Installation Documentation](https://kubernetes.io/docs/tasks/tools/)
  - kubectx and kubens, [Installation Documentation](https://github.com/ahmetb/kubectx#Installation)
  - [Docker](https://docs.docker.com/get-docker/), [Installation Documentation](https://docs.docker.com/engine/install/)
  - Kind CLI, [Installation Documentation](https://kind.sigs.k8s.io/docs/user/quick-start/#installation) 
- Infrastructure Requirements: 
  - Minimum of 8vCPUs and 8GB of RAM 
- Requirements for Hosting KubeSlice Controller 
  - Cluster Requirements: 1 Kubernetes Cluster 
  - Supported Kubernetes Version: [1.21](https://v1-21.docs.kubernetes.io/) and [1.22](https://v1-22.docs.kubernetes.io/) 
  - Supported Helm version: 3.7.0 
- Requirements for worker clusters: 
  - Minimum Clusters Required: 2 Kubernetes Clusters 
  - Node per Cluster: 1 Node per cluster 
  - Supported Kubernetes Version: [1.21](https://v1-21.docs.kubernetes.io/) and [1.22](https://v1-22.docs.kubernetes.io/) 
  - Supported Helm version: 3.7.0 

Prabhu used the kind cluster bash automation script that is available on the [GitHub repo](https://github.com/kubeslice/examples/tree/master/kind) for his live demo, 
even though you can manually construct a cluster and install the slice. This script sets up a Kubernetes 
cluster using [kind](https://kind.sigs.k8s.io/) and installs the necessary KubeSlice components. 

After installing the prerequisites mentioned above, you can follow along by directly cloning [the repository](https://github.com/kubeslice/examples) 
locally and running the kind.sh script. In order to test the multi-cluster functionality, the script 
additionally configures an iPerf client-server application. As an alternative, [Terraform](https://github.com/kubeslice/examples/tree/master/ec2) can be used to 
install KiND and Kubernetes on AWS EC2 instances.

***P.S. We are in the process of optimizing & simplifying these for a better user experience.If this is something you are interested in helping with, do hop on to the #kubeslice channel on the [Kubernetes slack](https://slack.k8s.io/).***

After testing the connectivity using iPerf, Prabhu deployed a simple web application called book-info using 
a yaml config file. This application displays information related to a book such as the author, year of 
publishing, reviews, ratings etc. on the browser. The config file used to deploy the application is 
available in the [kubeslice/examples](https://github.com/kubeslice/examples/tree/master/kind/bookinfo/config_files) repository on GitHub. 

The product page is deployed in the *worker cluster-1* and the book-info details, reviews, service exports, and 
ratings pages are deployed in *worker cluster-2*. The NodePort service is used to expose the product page 
deployed in worker cluster-1. Since the kind.sh script creates a book-info namespace created in each cluster, 
the http application will be automatically onboarded onto the Slice once we deploy the yaml file. Once created, 
it can be accessed across the slice and isolated from the other namespaces within the cluster - solving the noisy/nosey 
neighbor problem. Not only this, granular quotas can be set so as to avoid resource hogging by any particular application 
thus ensuring equitable distribution of resources in a multi-cluster, multi-tenant setup.

If you face any issue while following along or want to know more about the project, we encourage you to raise questions 
in the *#kubeslice* channel on the [Kubernetes slack](https://slack.k8s.io/) where we hang out. Additionally, the Office Hours are intended 
to be as interactive as possible and weâ€™d love you to join us on our next one to be held on 8th September 2022. 
To receive an invite for the next one in your inbox & stay updated with the latest goings-on in the project, [join our Google Group](https://groups.google.com/g/kubeslice-dev).

Until next time!