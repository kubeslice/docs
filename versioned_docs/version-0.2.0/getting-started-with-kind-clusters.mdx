# Getting Started with Kind Clusters

## Introduction 

This guide will walk you through successfully registering your kind
clusters with the KubeSlice Controller and creating a slice across the
clusters. We have worked to make this experience as seamless as
possible. If you have any comments, questions, or issues, please feel
free to join our [community](community).

The following sections help you to:

-   Install the KubeSlice Controller on the controller cluster
-   Register the worker clusters on the controller cluster and install
    the Slice Operator (Worker Operator) on the worker clusters
-   Create a slice
-   Test the connectivity between the worker clusters using the iPerf
    tool

The following diagram shows the topology of the KubeSlice installed on
kind clusters.

![alt](/images/Kindclusters-OS.png)

## Prerequisites
Ensure that you have the following environment set up that is required to install 
KubeSlice. In this environment, you must install the KubeSlice Controller and register 
clusters with it.

### Command Line Tools 
You need the following command line tools to install the KubeSlice.

|  Package Required   | Installation Instructions
|---------------------|--------------------------
| [Helm](https://helm.sh/) - The Package Manager for Kubernetes | https://helm.sh/docs/intro/install/ 
| [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/) - Kubernetes CLI | https://kubernetes.io/docs/tasks/tools/
| [kubectx and kubens](https://github.com/ahmetb/kubectx) |  https://github.com/ahmetb/kubectx#Installation
| [Docker](https://docs.docker.com/get-docker/) | https://docs.docker.com/engine/install/
| Kind CLI | https://kind.sigs.k8s.io/docs/user/quick-start/#installation


### Infrastructure Requirements
The following are the infrastructure requirements to install KubeSlice
components.

:::note
If you are on Ubuntu OS, then it is recommended to increase the `ulimt`
to 2048 or unlimited.
:::

|                                         |                           |
|-----------------------------------------|---------------------------|
| **Host Requirements**               | Minimum of 8vCPUs and 8GB of RAM  | 

:::note                                     
Ensure to modify the memory and CPU usage allowed to docker as described in           
https://docs.docker.com/desktop/windows/#resources. 
::: 

#### Requirements for Hosting KubeSlice Controller
   
:::info 
You can install the KubeSlice Controller on a cluster and also register the same cluster 
with the KubeSlice Controller.
:::

|                                         |                           |
|-----------------------------------------|---------------------------|
| **Cluster Requirements**                |     1 Kubernetes Cluster  |  
| **Supported Kubernetes Versions**       |   [1.21](https://v1-21.docs.kubernetes.io/) and [1.22](https://v1-22.docs.kubernetes.io/) |
| **Required Helm Version**               |  3.7.0  |


#### Requirements for Worker Clusters

|                                         |                           |
|-----------------------------------------|---------------------------|
| **Minimum Clusters Required**           | 2 Kubernetes Clusters    |
|  **Nodes Reserved for KubeSlice Components]** |   1 Node per cluster |
| **Supported Kubernetes Versions**       |   [1.21](https://v1-21.docs.kubernetes.io/) and [1.22](https://v1-22.docs.kubernetes.io/) |
| **Required Helm Version**               |  3.7.0  |

### Cluster Context Switching
You must change your `kubeconfig` context frequently to run
`kubectl `commands in each cluster. Using `kubectx` can make this
process easier and save you time.

To switch the context to a certain cluster, run the following command
with the corresponding cluster name:

```
kubectx <cluster name>
```

Expected Output

```
Switched to context "<cluster name>"
```

To return to the last used context, use the following command:

```
kubectx -
```

Expected Output

```
Switched to context "<previous context>"
```


### Preparing the Controller Cluster for Registration

Create a YAML file to prepare the controller cluster for registration by using the 
following template:

:::note 
The `networking `property is required for the namespace isolation
feature. By default, the kind cluster has the kindnet CNI setting, but it needs to be 
disabled for the namespace isolation feature to work. We install Calico instead for the
CNI network.
:::

:::note
To understand more about the configuration parameters, see
[https://kind.sigs.k8s.io/docs/user/configuration/](https://kind.sigs.k8s.io/docs/user/configuration/).
:::

:::warning
If you face memory issues with a **two-nodes** kind cluster, then use a
**single-node** kind cluster.
:::

```
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  # WARNING: It is _strongly_ recommended that you keep this the default
  # (127.0.0.1) for security reasons. However it is possible to change this.
  apiServerAddress: "127.0.0.1"
  # By default the API server listens on a random open port.
  # You may choose a specific port but probably don't need to in most cases.
  # Using a random port makes it easier to spin up multiple clusters.
  apiServerPort: 6443
  # By default kind takes kindnet CNI but we are disabling this to use netpol feature
  disableDefaultCNI: true # disable kindnet 
  podSubnet: 192.168.0.0/16 # set to Calico's default subnet
nodes:
  - role: control-plane
    image: kindest/node:v1.21.10@sha256:84709f09756ba4f863769bdcabe5edafc2ada72d3c8c44d6515fc581b66b029c
  - role: worker
    image: kindest/node:v1.21.10@sha256:84709f09756ba4f863769bdcabe5edafc2ada72d3c8c44d6515fc581b66b029c
    kubeadmConfigPatches:
      - |
        kind: JoinConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "kubeslice.io/node-type=gateway"
```

 Use the following template to create a single-node controller cluster. 

 ```
 kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  # WARNING: It is _strongly_ recommended that you keep this the default
  # (127.0.0.1) for security reasons. However it is possible to change this.
  apiServerAddress: "127.0.0.1"
  # By default the API server listens on a random open port.
  # You may choose a specific port but probably don't need to in most cases.
  # Using a random port makes it easier to spin up multiple clusters.
  apiServerPort: 6443
  # By default kind takes kindnet CNI but we are disabling this to use netpol feature
  disableDefaultCNI: true # disable kindnet 
  podSubnet: 192.168.0.0/16 # set to Calico's default subnet
nodes:
  - role: control-plane
    image: kindest/node:v1.21.10@sha256:84709f09756ba4f863769bdcabe5edafc2ada72d3c8c44d6515fc581b66b029c
    kubeadmConfigPatches:
      - |
        kind: InitConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "kubeslice.io/node-type=gateway"
 ```

### Applying the YAML File to Create the Controller Cluster 
Apply the YAML File to  create the controller cluster by running this command:

```
kind create cluster --name <Controller-Cluster-Name> --config kind-controller-cluster.yaml
```


### Preparing the Worker Cluster for Registration
Create a YAML file to prepare the worker cluster for registration by using the 
following template:

:::note 
The `networking `property is required for the namespace isolation
feature. By default, the kind cluster has the kindnet CNI setting, but it needs to be 
disabled for the namespace isolation feature to work. We install Calico instead for the
CNI network.
:::

:::note
To understand more about the configuration parameters, see
[https://kind.sigs.k8s.io/docs/user/configuration/](https://kind.sigs.k8s.io/docs/user/configuration/).
:::

:::warning
If you face memory issues with a **two-nodes** kind cluster, then use a
**single-node** kind cluster.
:::

```
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking: 
  # By default kind takes kindnet CNI but we are disabling this to use netpol feature
  disableDefaultCNI: true # disable kindnet 
  podSubnet: 192.168.0.0/16 # set to Calico's default subnet
nodes:
  - role: control-plane
    image: kindest/node:v1.21.10@sha256:84709f09756ba4f863769bdcabe5edafc2ada72d3c8c44d6515fc581b66b029c
  - role: worker
    image: kindest/node:v1.21.10@sha256:84709f09756ba4f863769bdcabe5edafc2ada72d3c8c44d6515fc581b66b029c  
    kubeadmConfigPatches:
      - |
        kind: JoinConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "kubeslice.io/node-type=gateway"    
```

Use the following template to create a single-node worker cluster.

```
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking: 
  # By default kind takes kindnet CNI but we are disabling this to use netpol feature
  disableDefaultCNI: true # disable kindnet 
  podSubnet: 192.168.0.0/16 # set to Calico's default subnet
nodes:
  - role: control-plane
    image: kindest/node:v1.21.10@sha256:84709f09756ba4f863769bdcabe5edafc2ada72d3c8c44d6515fc581b66b029c
    kubeadmConfigPatches:
      - |
        kind: InitConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "kubeslice.io/node-type=gateway"
```

### Applying the YAML File to Create the Worker Cluster
Apply the YAML File to create the worker cluster by running this
command:

**For worker cluster 1**

``` 
kind create cluster --name <Worker-Cluster-Name-1> --config kind-Worker-cluster.yaml
```

**For worker cluster 2**

```
kind create cluster --name <Worker-Cluster-Name-2> --config kind-Worker-cluster.yaml
```

### Installing Calico Networking and Network Security
Install
[Calico](https://projectcalico.docs.tigera.io/about/about-calico) to provide networking and
network security for kind clusters.

:::note
Install Calico only after creating the clusters.
:::

To install Calico on a kind cluster:
1. Install the operator on your cluster by using the following command:

```
kubectl create -f https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml
```

2. Download the custom resources required to configure Calico by using
the following command:

```
curl https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml -O
```

Running the above command downloads a file, which contains the following
content.

```
# This section includes base Calico installation configuration.
# For more information, see: https://projectcalico.docs.tigera.io/v3.23/reference/installation/api#operator.tigera.io/v1.Installation
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  # Configures Calico networking.
  calicoNetwork:
    # Note: The ipPools section cannot be modified post-install.
    ipPools:
    - blockSize: 26
      cidr: 192.168.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()

---

# This section configures the Calico API server.
# For more information, see: https://projectcalico.docs.tigera.io/v3.23/reference/installation/api#operator.tigera.io/v1.APIServer
apiVersion: operator.tigera.io/v1
kind: APIServer
metadata:
  name: default
spec: {}
```

3. Create the manifest to install Calico by using the following
command:

```
kubectl create -f custom-resources.yaml
```

4. Validate namespaces related to Calico by using the following
command:

```
kubectl get ns
```

Expected Output

```
NAME                   STATUS   AGE
calico-apiserver       Active   3d
calico-system          Active   3d
default                Active   3d
kube-node-lease        Active   3d
kube-public            Active   3d
kube-system            Active   3d
local-path-storage     Active   3d
tigera-operator        Active   3d
```

5. Validate the Calico pods by using the following command: 

```
kubectl get pods -n calico-system
```

Expected Output

```
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-59f859b79d-vbmqh   1/1     Running   1          3d
calico-node-nq7sp                          1/1     Running   0          3d
calico-node-rhw7h                          1/1     Running   0          3d
calico-node-tfqzp                          1/1     Running   0          3d
calico-typha-8b888f7d8-fx62t               1/1     Running   0          3d
calico-typha-8b888f7d8-vnb67               1/1     Running   0          3d
```

:::success
Calico networking is installed successfully. 
:::

### Adding the Helm Repo 
Add the KubeSlice helm repo in your machine where kind clusters are running using the following commands:

```
helm repo add kubeslice https://kubeslice.github.io/charts/
helm repo update
```

### Validating the Helm Repo 
Validate the helm repo by using the following command:

```
helm search repo kubeslice
```

Expected Output

```
NAME                            CHART VERSION   APP VERSION     DESCRIPTION                                       
kubeslice/cert-manager          v1.7.0          v1.7.0          A Helm chart for cert-manager                     
kubeslice/istio-base            1.13.3          1.13.3          Helm chart for deploying Istio cluster resource...
kubeslice/istio-discovery       1.13.3          1.13.3          Helm chart for istio control plane                
kubeslice/kubeslice-controller  0.2.0           0.3.6           A Helm chart for kubeslice-controller             
kubeslice/kubeslice-worker      0.2.0           0.4.1           KubeSlice Operator
```

## Installing the KubeSlice Controller 

### Prerequisites for KubeSlice Controller 

### Installing the Certificate Manager 

### Validating the Certificate Manager

### Creating the Controller YAML File

### Applying the Controller YAML File 

### Validating the Controller Installation

### Creating a Project Namespace

#### Creating a Project YAML File

#### Applying the Project YAML File

#### Validating the Project

## Registering the Worker Clusters

### Creating the Cluster Registration YAML File

### Applying the Cluster Registration YAML File

### Validating the Registered Clusters

### Installing the Slice Operator 

#### Getting the Secrets of the Registered Cluster from the Controller Cluster 

### Creating the Slice Operator YAML file 


##### Get the Node IPs of Worker Clusters

### Applying the Slice Operator Values File 

### Validating the Slice Operator Installation 

## Creating a Slice 

## Creating the Slice YAML File 

#### Slice Configuration 

### Applying the Slice Configuration on the Controller Cluster 

## Validating the Slice Configuration from the Controller Cluster 

## Onboarding Namespaces 

## Isolating Namespaces 

## Testing the Network Connectivity between the Worker Clusters

## Uninstalling the KubeSlice 